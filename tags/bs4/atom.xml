<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: bs4 | 杂七杂八记一下]]></title>
  <link href="http://lixxu.me/tags/bs4/atom.xml" rel="self"/>
  <link href="http://lixxu.me/"/>
  <updated>2014-07-29T21:21:09+08:00</updated>
  <id>http://lixxu.me/</id>
  <author>
    <name><![CDATA[Lix Xu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[抓取 <<码农故事>> 文本]]></title>
    <link href="http://lixxu.me/blog/2014/07/25/get-coder-story-text-from-web/"/>
    <updated>2014-07-25T08:46:39+08:00</updated>
    <id>http://lixxu.me/blog/2014/07/25/get-coder-story-text-from-web</id>
    <content type="html"><![CDATA[<p>偶然看到这样一篇关于码农的小说, 感觉还可以, 就想收集下来慢慢看.
但是都是网页而且还挺多, 就看看有没有整理好的全本, 搜了一下没找到.</p>

<p>接着就手动复制粘贴, 贴了几篇感觉太枯燥, 就琢磨着如何使用 <code>requests</code> 和 <code>BeautifulSoup</code> 来提取文本.</p>

<p>很少使用 <code>BeautifulSoup</code>, 所以临时去看了一下文档, 开始走起.</p>

<p>1. 先用浏览器的 <code>审查元素</code> 获取分页的页面结构, 这里我们只需要 <code>标题</code> 和 <code>链接地址</code> 即可</p>

<p><img class="<a" src="href="http://lixxu.qiniudn.com/coder_page.png">http://lixxu.qiniudn.com/coder_page.png</a>&#8221;></p>

<!--more-->


<p>2. 分析页面内容抓取 <code>标题</code> 和 <code>链接地址</code>, 还一并抓取当前的章节, 否则就没法知道顺序了</p>

<p>3. 根据获取的链接抓取全文</p>

<p>4. 分析全文, 在 <code>entry</code> 下面的 <code>p</code> 和 <code>span</code> 里, 一开始我以为都在 <code>p</code> 结构里, 结果最后发现有的章节是空的, 看了下空的章节的结构, 原来用的是 <code>&lt;div&gt;&lt;span&gt;...&lt;/span&gt;&lt;/div&gt;</code> 的结构</p>

<p><img class="<a" src="href="http://lixxu.qiniudn.com/coder_post.png">http://lixxu.qiniudn.com/coder_post.png</a>&#8221;></p>

<p><img class="<a" src="href="http://lixxu.qiniudn.com/coder_post2.png">http://lixxu.qiniudn.com/coder_post2.png</a>&#8221;></p>

<p>5. 这些都确定了后就可以开工了, 当然我是逐步确定的. 我的流程是: 按章节写到单独的文件里, 最后再合并. 好了, <code>Talk is cheap, show me the code.</code>:</p>

<pre><code class="python coder_story.py">#!/usr/bin/env python
#-*- coding: utf-8 -*-

import os
import os.path
import requests
from bs4 import BeautifulSoup
import re

digit_re = re.compile('(\d+-?\d*)')
url = r'http://blog.jobbole.com/tag/%E7%A0%81%E5%86%9C%E6%95%85%E4%BA%8B/page/{}/'
if not os.path.exists('coder_story'):
    os.makedirs('coder_story')

os.chdir('coder_story')
for page in (1, 2, 3):
    r = requests.get(url.format(page))
    # soup = BeautifulSoup(r.content)  # lxml未安装时用这行
    soup = BeautifulSoup(r.content, 'lxml')
    for meta_title in soup.find_all('a', class_='meta-title'):
        # 我们要的都必须有title
        if 'title' not in meta_title.attrs:
            continue

        # 不是这个小说不要
        if u'老码农原创小说' not in meta_title['title']:
            continue

        print(meta_title)  # 打印一下标题
        href = meta_title.get('href')  # 获取文章链接
        # 获取章节序号
        found = digit_re.findall(meta_title['title'])
        if not found:
            continue

        # 个位数章节前补0, 为了后面的合并
        page_no = found[0]
        if int(page_no.split('-')[0]) &lt; 10:
            page_no = '0' + page_no

        # 获取文章的内容
        r2 = requests.get(href)
        # s2 = BeautifulSoup(r2.content)  # lxml未安装时用这行
        s2 = BeautifulSoup(r2.content, 'lxml')
        entry = s2.find('div', class_='entry')
        with open('{}.txt'.format(page_no), 'w') as f:
            # 内容在 &lt;p&gt;...&lt;/p&gt; 或 &lt;div&gt;&lt;span&gt;...&lt;/span&gt;&lt;/div&gt;里
            for p in entry.find_all(['p', 'span']):
                # copyright-meta部分, 跳过
                if p.get('class'):
                    continue

                # 空白行, 文件里写两行空白, 便于阅读
                text = p.get_text()
                if not text:
                    f.write('\r\n\r\n')
                    continue

                f.write(text.encode('utf-8') + '\r\n')
                # "待续" 的地方, 接着就是另一章了, 插入空行
                if text.endswith(u'&lt;\u5f85\u7eed&gt;'):
                    f.write('\r\n')
</code></pre>

<p>6. 合并到一个文件, 用 <code>Python</code> 碰到编码错误, 时间紧, 就用批处理好了, 还简单
<code>
copy *.txt 码农故事.txt
</code></p>

<p>7. 打完收工.</p>

<p>后记:</p>

<p>就在合并时, 看到最后的一句话, 马上吐出一升老血:</p>

<p><img class="<a" src="href="http://lixxu.qiniudn.com/coder_end.png">http://lixxu.qiniudn.com/coder_end.png</a>&#8221;></p>

<p><img class="<a" src="href="http://lixxu.qiniudn.com/tuxie.gif">http://lixxu.qiniudn.com/tuxie.gif</a>&#8221;></p>

<p>不过, 在 <code>kindle</code> 上还是 txt 的效果好一些, pdf 转换的有些乱码, 不完美.</p>

<p><img class="<a" src="href="http://lixxu.qiniudn.com/kindle_bug.png">http://lixxu.qiniudn.com/kindle_bug.png</a>&#8221;></p>

<p>附下载地址:</p>

<p>1. TXT <a href="http://lixxu.qiniudn.com/%E7%A0%81%E5%86%9C%E6%95%85%E4%BA%8B.txt">http://lixxu.qiniudn.com/%E7%A0%81%E5%86%9C%E6%95%85%E4%BA%8B.txt</a></p>

<p>2. PDF <a href="http://lixxu.qiniudn.com/%E7%A0%81%E5%86%9C%E6%95%85%E4%BA%8B.pdf">http://lixxu.qiniudn.com/%E7%A0%81%E5%86%9C%E6%95%85%E4%BA%8B.pdf</a></p>

<!--more-->

]]></content>
  </entry>
  
</feed>